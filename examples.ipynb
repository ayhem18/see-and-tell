{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples of the code's usage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the embeddings for the Big Bang Theory Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bouab\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\cntell-xeiog5pY-py3.10\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bouab\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\cntell-xeiog5pY-py3.10\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 256, 256])\n",
      "torch.Size([3, 256, 256])\n",
      "torch.Size([3, 256, 256])\n",
      "torch.Size([3, 256, 256])\n",
      "torch.Size([3, 256, 256])\n",
      "torch.Size([3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# simple set the path and call the function from the face_recognition file\n",
    "from pathlib import Path\n",
    "import os\n",
    "HOME = os.getcwd()\n",
    "BIG_BANG_THEORY_DIR = Path(os.path.join(HOME, 'src', 'dataset', 'BigBangTheory')) # this piece of code assumes the data is present in the local file system.\n",
    "\n",
    "from src.face.face_recognition import build_classes_embeddings\n",
    "\n",
    "bbt_embeddings = build_classes_embeddings(BIG_BANG_THEORY_DIR, 'src/TBBT_embeddings_16_256.json', save_faces=os.path.join('src', 'cropped_images'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for char, emb in bbt_embeddings.items():\n",
    "#     print(len(emb))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the embeddings of the House of Cards Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's do the same for 'HOUSE OF CARDS' series\n",
    "from pathlib import Path\n",
    "import os\n",
    "HOME = os.getcwd()\n",
    "HOUSE_OF_CARDS_DIR = Path(os.path.join(HOME, 'dataset', 'HouseOfCards'))\n",
    "\n",
    "# hoc_embeddings = build_classes_embeddings(HOUSE_OF_CARDS_DIR, 'HOC_embeddings.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first define the Path to the data\n",
    "BIG_BANG_THEORY_DIR = Path(os.path.join(HOME, 'dataset', 'BigBangTheory')) # this piece of code assumes the data is present in the local file system.\n",
    "RESIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# building the embeddings for the big bang theory cast members\n",
    "# from glob import glob\n",
    "# from collections import Counter\n",
    "# from helper_functions import all_images_in_directory \n",
    "# all_images = all_images_in_directory(Path(BIG_BANG_THEORY_DIR))\n",
    "# # iterate through the entire \n",
    "# classes_paths = Counter()\n",
    "# for img_path in all_images:\n",
    "#     # extract the class\n",
    "#     img_path = Path(img_path)\n",
    "#     # the class will simply be the name of the parent directory\n",
    "#     img_class = os.path.basename(img_path.parent)\n",
    "#     if img_class not in classes_paths: \n",
    "#         # create the list if the class was not yet added\n",
    "#         classes_paths[img_class] = [img_path]\n",
    "#     else:\n",
    "#         # append the current \n",
    "#         classes_paths[img_class].append(img_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = Counter()\n",
    "\n",
    "# from face_recognition import get_embeddings\n",
    "# for char in classes_paths.keys():\n",
    "#     save_path = os.path.join(HOME, 'cropped_images_2', char)\n",
    "#     embeddings[char] = get_embeddings(images=classes_paths[char], keep_all=False, save_path=save_path, save_faces=True)\n",
    "\n",
    "# import json\n",
    "# with open('TBBT_EMBS_2.json', 'w') as fp:\n",
    "#     json.dump(dict(embeddings), fp, indent=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recognizing the faces in a BBT scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# first load the embeddings before hand for better performance\n",
    "with open(os.path.join(HOME, 'EMBEDDINGS.json')) as f:\n",
    "  bbt_embeddings = json.load(f)\n",
    "\n",
    "print(list(bbt_embeddings.keys()))\n",
    "from face_recognition import recognize_faces\n",
    "path = Path(os.path.join(HOME, 'scapped_images', 'test_im1.png')) # replace with any path needed\n",
    "recognize_faces(path, embeddings=bbt_embeddings, possible_classes=list(bbt_embeddings.keys()), display=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is preferable to use a file path rather than a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try numpy arrays\n",
    "import cv2\n",
    "img = cv2.imread(str(path))\n",
    "recognize_faces(img, embeddings=bbt_embeddings, possible_classes=list(bbt_embeddings.keys()), display=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7bb0ce5cb6b092cde9f0ba713d915425207ed6ea08d3ede97530b87c251a3aee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
