{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install nltk\n",
    "# ! pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bouab\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\cntell-xeiog5pY-py3.10\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bouab\\DEV\\see-and-tell\\src\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-06 17:47:21 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| constituency | wsj       |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2023-05-06 17:47:23 INFO: Using device: cuda\n",
      "2023-05-06 17:47:23 INFO: Loading: tokenize\n",
      "2023-05-06 17:47:27 INFO: Loading: pos\n",
      "2023-05-06 17:47:27 INFO: Loading: lemma\n",
      "2023-05-06 17:47:27 INFO: Loading: constituency\n",
      "2023-05-06 17:47:28 INFO: Loading: depparse\n",
      "2023-05-06 17:47:29 INFO: Loading: sentiment\n",
      "2023-05-06 17:47:29 INFO: Loading: ner\n",
      "2023-05-06 17:47:30 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "import os\n",
    "HOME = os.getcwd()\n",
    "print(HOME)\n",
    "\n",
    "from stanza.pipeline.core import DownloadMethod\n",
    "NLP = stanza.Pipeline('en', download_method=DownloadMethod.REUSE_RESOURCES, use_gpu=True) # to avoid downloading the models every time\n",
    "# nlp is a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['The woman in the garden is reading a book next to her husband', 'A man in a blue shirt is talking to another man', \"A young girl is having breakfast with her mother\"]\n",
    "final_str = \". \".join(sentences)\n",
    "# let's see how things go now !!\n",
    "doc = NLP(final_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = doc.sentences\n",
    "tree = sentences[0].constituency\n",
    "c = tree.children[0].children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first 2 in general are a NP and VP\n",
    "# let's check the first component: NP\n",
    "np1 = c[0]\n",
    "res = stanza.models.constituency.parse_tree.Tree.get_compound_constituents([np1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Tree in module stanza.models.constituency.parse_tree:\n",
      "\n",
      "class Tree(stanza.models.common.stanza_object.StanzaObject)\n",
      " |  Tree(label=None, children=None)\n",
      " |  \n",
      " |  A data structure to represent a parse tree\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Tree\n",
      " |      stanza.models.common.stanza_object.StanzaObject\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __format__(self, spec)\n",
      " |      Turn the tree into a string representing the tree\n",
      " |      \n",
      " |      Note that this is not a recursive traversal\n",
      " |      Otherwise, a tree too deep might blow up the call stack\n",
      " |      \n",
      " |      There is a type specific format:\n",
      " |        O       -> one line PTB format, which is the default anyway\n",
      " |        L       -> open and close brackets are labeled, spaces in the tokens are replaced with _\n",
      " |        P       -> pretty print over multiple lines\n",
      " |        V       -> surround lines with <s>...</s>, don't print ROOT, and turn () into L/RBKT\n",
      " |        ?       -> spaces in the tokens are replaced with ? for any value of ? other than OLP\n",
      " |                   warning: this may be removed in the future\n",
      " |        ?{OLPV} -> specific format AND a custom space replacement\n",
      " |        Vi      -> add an ID to the <s> in the V format.  Also works with ?Vi\n",
      " |  \n",
      " |  __init__(self, label=None, children=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  all_leaves_are_preterminals(self)\n",
      " |      Returns True if all leaves are under preterminals, False otherwise\n",
      " |  \n",
      " |  count_unary_depth(self)\n",
      " |  \n",
      " |  depth(self)\n",
      " |  \n",
      " |  is_leaf(self)\n",
      " |  \n",
      " |  is_preterminal(self)\n",
      " |  \n",
      " |  leaf_labels(self)\n",
      " |      Get the labels of the leaves\n",
      " |  \n",
      " |  pretty_print(self, normalize=None)\n",
      " |      Print with newlines & indentation on each line\n",
      " |      \n",
      " |      Preterminals and nodes with all preterminal children go on their own line\n",
      " |      \n",
      " |      You can pass in your own normalize() function.  If you do,\n",
      " |      make sure the function updates the parens to be something\n",
      " |      other than () or the brackets will be broken\n",
      " |  \n",
      " |  prune_none(self)\n",
      " |      Return a copy of the tree, eliminating all nodes which are in one of two categories:\n",
      " |          they are a preterminal -NONE-, such as appears in PTB\n",
      " |            *E* shows up in a VLSP dataset\n",
      " |          they have been pruned to 0 children by the recursive call\n",
      " |  \n",
      " |  remap_constituent_labels(self, label_map)\n",
      " |      Copies the tree with some labels replaced.\n",
      " |      \n",
      " |      Labels in the map are replaced with the mapped value.\n",
      " |      Labels not in the map are unchanged.\n",
      " |  \n",
      " |  remap_words(self, word_map)\n",
      " |      Copies the tree with some labels replaced.\n",
      " |      \n",
      " |      Labels in the map are replaced with the mapped value.\n",
      " |      Labels not in the map are unchanged.\n",
      " |  \n",
      " |  replace_words(self, words)\n",
      " |      Replace all leaf words with the words in the given list (or iterable)\n",
      " |      \n",
      " |      Returns a new tree\n",
      " |  \n",
      " |  reverse(self)\n",
      " |      Flip a tree backwards\n",
      " |      \n",
      " |      The intent is to train a parser backwards to see if the\n",
      " |      forward and backwards parsers can augment each other\n",
      " |  \n",
      " |  simplify_labels(self, pattern=re.compile('[-=#]'))\n",
      " |      Return a copy of the tree with the -=# removed\n",
      " |      \n",
      " |      Leaves the text of the leaves alone.\n",
      " |  \n",
      " |  visit_preorder(self, internal=None, preterminal=None, leaf=None)\n",
      " |      Visit the tree in a preorder order\n",
      " |      \n",
      " |      Applies the given functions to each node.\n",
      " |      internal: if not None, applies this function to each non-leaf, non-preterminal node\n",
      " |      preterminal: if not None, applies this functiion to each preterminal\n",
      " |      leaf: if not None, applies this function to each leaf\n",
      " |      \n",
      " |      The functions should *not* destructively alter the trees.\n",
      " |      There is no attempt to interpret the results of calling these functions.\n",
      " |      Rather, you can use visit_preorder to collect stats on trees, etc.\n",
      " |  \n",
      " |  yield_preterminals(self)\n",
      " |      Yield the preterminals one at a time in order\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  get_common_words(trees, num_words)\n",
      " |      Walks over all of the trees and gets the most frequently occurring words.\n",
      " |  \n",
      " |  get_compound_constituents(trees, separate_root=False)\n",
      " |  \n",
      " |  get_constituent_counts(trees)\n",
      " |      Walks over all of the trees and gets the count of the unique constituent names from the trees\n",
      " |  \n",
      " |  get_rare_words(trees, threshold=0.05)\n",
      " |      Walks over all of the trees and gets the least frequently occurring words.\n",
      " |      \n",
      " |      threshold: choose the bottom X percent\n",
      " |  \n",
      " |  get_root_labels(trees)\n",
      " |  \n",
      " |  get_unique_constituent_labels(trees)\n",
      " |      Walks over all of the trees and gets all of the unique constituent names from the trees\n",
      " |  \n",
      " |  get_unique_tags(trees)\n",
      " |      Walks over all of the trees and gets all of the unique tags from the trees\n",
      " |  \n",
      " |  get_unique_words(trees)\n",
      " |      Walks over all of the trees and gets all of the unique words from the trees\n",
      " |  \n",
      " |  write_treebank(trees, out_file, fmt='{}')\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from stanza.models.common.stanza_object.StanzaObject:\n",
      " |  \n",
      " |  add_property(name, default=None, getter=None, setter=None) from builtins.type\n",
      " |      Add a property accessible through self.{name} with underlying variable self._{name}.\n",
      " |      Optionally setup a setter as well.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from stanza.models.common.stanza_object.StanzaObject:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(stanza.models.constituency.parse_tree.Tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('VBZ', 'is'), ('VBG', 'reading'), ('DT', 'a'), ('NN', 'book'), ('RB', 'next'), ('IN', 'to'), ('PRP$', 'her'), ('NN', 'husband')]\n",
      "[('VBG', 'reading'), ('DT', 'a'), ('NN', 'book'), ('RB', 'next'), ('IN', 'to'), ('PRP$', 'her'), ('NN', 'husband')]\n"
     ]
    }
   ],
   "source": [
    "# let's try to understand how to identify expressions that could represent humans\n",
    "from nltk.corpus import wordnet as wn\n",
    "PEOPLE = wn.synset('people.n.01')\n",
    "PERSON = wn.synset('person.n.01')\n",
    "\n",
    "def words_tags(tree):\n",
    "    return [(x.label, x.children[0].label) for x in tree.yield_preterminals()]\n",
    "\n",
    "print(words_tags(c[1]))\n",
    "print(words_tags(c[1].children[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def person_word(word:str, threshold: float=0.2, synset_reference=None) -> bool:\n",
    "    # this function will return 2 boolean values:\n",
    "    # first if the word could possibly mean the word \"person\"     \n",
    "\n",
    "    # first extract the possible meanings of such word\n",
    "    if synset_reference is None:\n",
    "        synset_reference = PERSON # this way we can use PEOPLE as well with the same function\n",
    "\n",
    "    is_person = False\n",
    "    for meaning in wn.synsets(word):\n",
    "        if meaning.path_similarity(PERSON) >= threshold:\n",
    "            is_person = True\n",
    "            break\n",
    "    \n",
    "    return is_person\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a woman who has given birth to a child (also used as a term of address to your mother)\n",
      "0.14285714285714285\n",
      "a stringy slimy substance consisting of yeast cells and bacteria; forms during fermentation and is added to cider or wine to produce vinegar\n",
      "0.125\n",
      "a term of address for an elderly woman\n",
      "0.2\n",
      "a term of address for a mother superior\n",
      "0.16666666666666666\n",
      "a condition that is the inspiration for an activity or situation\n",
      "0.09090909090909091\n",
      "care for like a mother\n",
      "0.1\n",
      "make children\n",
      "0.125\n"
     ]
    }
   ],
   "source": [
    "for meaning in wn.synsets('mother'):\n",
    "    print(meaning.definition())\n",
    "    print(meaning.path_similarity(PERSON))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(word: str , tag: str):\n",
    "    try:\n",
    "        pos = tag.lower()[0]\n",
    "        lemmas = wn._morphy(word, pos)\n",
    "        return min(lemmas, key=len) if lemmas else word\n",
    "    except:\n",
    "        # assume it is a noun\n",
    "        lemmas = wn._morphy(word, 'n')\n",
    "        return min(lemmas, key=len) if lemmas else word\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_person_words(noun_phrase: list[tuple[str, str]], known_person_words: set):\n",
    "    # first convert all the words to their lemmas\n",
    "    lemmas = [(t[0], lemmatize(t[1], t[0])) for t in noun_phrase]    \n",
    "    person_lists = [t[1] for t in lemmas if t[0] == 'NN' and (t[1] in known_person_words or \n",
    "                    person_word(t[1], synset_reference=PERSON) or person_word(t[1], synset_reference=PEOPLE))]\n",
    "\n",
    "    # make sure to add the words that could possibly mean PERSON to the set\n",
    "    known_person_words.update(person_lists)\n",
    "    return person_lists, known_person_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_NP_components(root, person_np: set=None):\n",
    "    # this function is supposed to return the largest NPs including exactly one word with a close meaning to 'PERSON'  \n",
    "    if person_np is None:\n",
    "        person_np = set()\n",
    "    \n",
    "    result = []\n",
    "    # only consider Noun Phrases\n",
    "    if root.label == 'NP':\n",
    "        # if the current root reprsents a Noun Phrase, \n",
    "        noun_phrase = words_tags(root)\n",
    "        # extract the number of words that represent a PERSON\n",
    "        num_persons, person_np = num_person_words(noun_phrase, person_np)\n",
    "\n",
    "        if len(num_persons) == 1:\n",
    "            result.append(noun_phrase)\n",
    "        \n",
    "        elif len(num_persons) >= 2: \n",
    "            for child in root.children:\n",
    "                result.extend(get_NP_components(child, person_np))\n",
    "\n",
    "    else:\n",
    "        for child in root.children:\n",
    "            result.extend(get_NP_components(child, person_np))\n",
    "\n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_text(l: list[tuple[str, str]], filter):\n",
    "    if filter:\n",
    "        return \" \".join([t[1] for t in l if t[0] in ['NN', 'JJ']]).strip().lower() \n",
    "    return \" \".join([t[1] for t in l]).strip().lower()\n",
    "\n",
    "\n",
    "def extract_NP_text(text: str, nlp_object=None, plain_text:bool=True, filter:bool=True):\n",
    "    if nlp_object is None:\n",
    "        nlp_object = NLP\n",
    "    \n",
    "    doc = nlp_object(text)\n",
    "    np_components = []\n",
    "    person_words = set()\n",
    "    # iterate through sentences\n",
    "    for s in doc.sentences:\n",
    "        tree = s.constituency\n",
    "        c = tree.children[0]\n",
    "        np_components.append(get_NP_components(c, person_words))\n",
    "\n",
    "    if plain_text:\n",
    "        return [[convert_to_text(t, filter=filter) for t in component] for component in np_components]\n",
    "    \n",
    "    return np_components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The woman in the garden is reading a book next to her husband. A man in a blue shirt is talking to another man. A young girl is having breakfast with her mother. One boy and one man were hitting an old woman\n",
      "[['woman garden', 'husband'], ['man blue shirt', 'man'], ['young girl', 'mother'], ['boy', 'man', 'old woman']]\n"
     ]
    }
   ],
   "source": [
    "sentences = ['The woman in the garden is reading a book next to her husband', \n",
    "             'A man in a blue shirt is talking to another man', \n",
    "             \"A young girl is having breakfast with her mother\", \n",
    "             \"One boy and one man were hitting an old woman\"]\n",
    "final_str = \". \".join(sentences)\n",
    "print(final_str)\n",
    "results = extract_NP_text(final_str, plain_text=True, filter=True)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for res, s in zip(results, sentences):\n",
    "#     print(s)\n",
    "#     print(\"Person Noun Phrases\")\n",
    "#     for np in res:\n",
    "#         print(\" \".join([w[1] for w in np]))\n",
    "\n",
    "#     print(\"#\" * 100)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "HOME = os.getcwd()\n",
    "import json\n",
    "with open(os.path.join(HOME,'TBBT_embeddings_16_256.json')) as f:\n",
    "  bbt_embeddings = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import beyond top-level package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexp\u001b[39;00m \u001b[39mimport\u001b[39;00m get_caption\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mface\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mface_recognition\u001b[39;00m \u001b[39mimport\u001b[39;00m recognize_faces\n\u001b[0;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_result_image\u001b[39m(image_path):\n\u001b[0;32m      4\u001b[0m     \u001b[39m# extract the caption\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bouab\\DEV\\see-and-tell\\src\\experimental\\exp.py:5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrequests\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdescribe\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframe\u001b[39;00m \u001b[39mimport\u001b[39;00m FrameDescriptor\n\u001b[0;32m      7\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_caption\u001b[39m(image_path: \u001b[39mstr\u001b[39m):\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m     \u001b[39m# Initialize the FrameDescriptor class\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     frame_desc \u001b[39m=\u001b[39m FrameDescriptor(\n\u001b[0;32m     11\u001b[0m         model_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmicrosoft/git-large-r-textcaps\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     12\u001b[0m         use_gpu\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     )\n",
      "\u001b[1;31mImportError\u001b[0m: attempted relative import beyond top-level package"
     ]
    }
   ],
   "source": [
    "from experimental.exp import get_caption\n",
    "from face.face_recognition import recognize_faces\n",
    "def get_result_image(image_path):\n",
    "    # extract the caption\n",
    "    caption = get_caption(image_path)\n",
    "    # get the faces present in the image \n",
    "    o1 = recognize_faces(image_path, embeddings=bbt_embeddings, possible_classes=list(bbt_embeddings.keys()), display=False)    \n",
    "    print(caption)\n",
    "    print(o1)\n",
    "    return caption, o1\n",
    "\n",
    "f = os.path.join(HOME, 'frames')\n",
    "frames = os.listdir(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bouab\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\cntell-xeiog5pY-py3.10\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a man in a blue shirt with the number 73 on it\n",
      "[['sheldon', array([720.093017578125, 102.15438079833984, 837.06201171875, 275.375],\n",
      "      dtype=object)]]\n",
      "a man in a blue shirt with the number 73 on it\n",
      "[['sheldon', array([685.4306030273438, 107.30406951904297, 804.5145263671875,\n",
      "       277.11749267578125], dtype=object)]]\n",
      "a man in a blue shirt with the number 73 on it\n",
      "[['sheldon', array([616.29150390625, 126.95941162109375, 739.9351196289062,\n",
      "       306.815673828125], dtype=object)]]\n",
      "a man in a blue shirt is bending over a white board that says \" let's do\n",
      "[['penny', array([495.4705505371094, 128.3004913330078, 545.2845458984375,\n",
      "       191.6173095703125], dtype=object)]]\n",
      "a man and woman stand in front of a white board that says \" the word \" on it\n",
      "[['penny', array([600.624755859375, 136.38824462890625, 651.2451171875,\n",
      "       202.7168426513672], dtype=object)]]\n",
      "a man and a woman stand in front of a white board that says \" the word \" on\n",
      "[]\n",
      "a man and woman are standing in front of a white board that says \" c. c.\n",
      "[]\n",
      "a man in a blue shirt with the number 13 on it\n",
      "[['amy', array([794.7205810546875, 235.90184020996094, 825.148193359375,\n",
      "       276.3747253417969], dtype=object)]]\n",
      "a man and a woman are standing in front of a white board that says \" c \" on\n",
      "[['leonard', array([811.4470825195312, 95.06720733642578, 873.2039794921875,\n",
      "       180.60411071777344], dtype=object)]]\n",
      "a man in a jacket and glasses sits in a chair in front of a bookcase that says\n",
      "[['leonard', array([532.877197265625, 148.51690673828125, 656.6834106445312,\n",
      "       313.33380126953125], dtype=object)]]\n",
      "a man and woman are drawing on a white board in a living room.\n",
      "[['bernadette', array([498.82415771484375, 182.71617126464844, 534.2024536132812,\n",
      "       237.98898315429688], dtype=object)], ['amy', array([788.1454467773438, 96.18675231933594, 823.7026977539062,\n",
      "       168.50164794921875], dtype=object)]]\n",
      "a man and woman are drawing on a white board.\n",
      "[['amy', array([805.125, 102.94965362548828, 846.38916015625, 174.30885314941406],\n",
      "      dtype=object)]]\n",
      "a woman sitting on a couch with a box of drinks on the table.\n",
      "[['amy', array([455.72564697265625, 74.2933578491211, 584.0984497070312,\n",
      "       242.30247497558594], dtype=object)]]\n",
      "a man sitting in a chair with a book shelf behind him.\n",
      "[['leonard', array([514.1798706054688, 104.80033111572266, 645.664306640625,\n",
      "       261.8083801269531], dtype=object)]]\n",
      "a man is drawing a square on a white board.\n",
      "[['amy', array([519.0037231445312, 175.24462890625, 614.4978637695312,\n",
      "       340.1865234375], dtype=object)]]\n",
      "a man is sitting in a chair and is wearing glasses and a jacket that says \" new york\n",
      "[['leonard', array([466.45977783203125, 99.00739288330078, 612.18359375,\n",
      "       274.1148986816406], dtype=object)]]\n",
      "a man is drawing a square on a white board with arrows on it.\n",
      "[['sheldon', array([736.9624633789062, 182.31396484375, 815.4859619140625,\n",
      "       312.6865539550781], dtype=object)]]\n",
      "a man sitting in a chair with a bookcase behind him.\n",
      "[['leonard', array([466.7824401855469, 99.67796325683594, 610.2178955078125,\n",
      "       256.5967712402344], dtype=object)]]\n",
      "a man is drawing a picture on a white board with the word ctc on it.\n",
      "[]\n",
      "a man in a jacket is sitting in a chair with a bookcase behind him.\n",
      "[['leonard', array([439.3456726074219, 112.86385345458984, 583.605224609375,\n",
      "       284.5859069824219], dtype=object)]]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "stop\n",
    "# results = [get_result_image(os.path.join(f, frames[i])) for i in range(0, 40, 2)]\n",
    "# let's take 20 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions, preds = list(map(list, zip(*results)))\n",
    "# let's save the results\n",
    "# with open( \"captions.p\", \"wb\" ) as f:\n",
    "# \tpickle.dump(captions, f)\n",
    "\n",
    "# with open( \"preds.p\", \"wb\" ) as f:\n",
    "# \tpickle.dump(preds, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def char_names_predictions(face_predictions: list[list[list[str, np.array]]]):\n",
    "    # first extract only the character names as needed\n",
    "    char_names  = [[l[0] for l in p] for p in face_predictions]\n",
    "    return char_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_captions_class_matrix(captions: list[list[str]], face_predictions: list[list[str]]):\n",
    "    class_np_counter = Counter() # to save the frequency of each np with each class\n",
    "    np_components = set()\n",
    "    np_class_counter = {} # to save which classes each np was seen with\n",
    "    for cap_list, pred_list in captions, face_predictions:\n",
    "        if len(cap_list) == len(pred_list):\n",
    "            # iterate through both captions and predictions and\n",
    "            for cap, pred in zip(cap_list, pred_list):\n",
    "                # first add each part of the caption to the np_components\n",
    "                np_components.update(np_components.split(\" \"))\n",
    "                if pred not in class_np_counter:\n",
    "                    class_np_counter[pred] = Counter() # a dictionary for each of the classes        \n",
    "                # increase the frequency of each term in the caption, in the dictionary associated with the class\n",
    "                class_np_counter[pred].update(dict([(word, 1) for word in cap.split(\" \")]))\n",
    "                # add the class to the occurences of each of the np in the captions\n",
    "                for np in cap.split(\" \"):\n",
    "                    if np not in np_class_counter:\n",
    "                        np_class_counter[np] = set() \n",
    "                    np_class_counter[np].add(pred)\n",
    "            \n",
    "    return class_np_counter, np_class_counter, np_components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def find_decided_captions(captions: list[str], face_predictions: list[list[str]]):\n",
    "    # this function will just return the captions and face_predictions with length 1\n",
    "    temp_list =  [(c[0], fp[0]) for c, fp in zip(captions, face_predictions) if len(c) == len(fp) == 1]\n",
    "    # convert the list of tuples to 2 lists\n",
    "    captions, predictions = list(map(list, zip(*temp_list)))\n",
    "    # build a counter to map each class to its decided captions\n",
    "    decisive_class_np_counter = Counter()\n",
    "    for c, pred in zip(captions, predictions):\n",
    "        if pred not in decisive_class_np_counter:\n",
    "            decisive_class_np_counter[pred] = Counter()\n",
    "        decisive_class_np_counter[pred].update(dict([(w, 1) for w in c.split(\" \")]))\n",
    "    return decisive_class_np_counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def np_class_score(np: str, class_prediction: str, class_np_counter: Counter, np_class_counter: Counter, decided_class_np: Counter):\n",
    "    def word_class_score(word: str):\n",
    "        # first let's build the numerator:1 + the number of occurences of the word with the class pred + the number of times it was seen in a decisive prediction\n",
    "        numerator = 1 + (class_np_counter[class_prediction][word] if word in class_np_counter[class_prediction] else 0) + \\\n",
    "                        (decided_class_np[class_prediction][word] if word in decided_class_np else 0)\n",
    "        # the denominator: the number of unique classes the word was associated with + 1 \n",
    "        denominator = 1 + len(np_class_counter[word]) \n",
    "\n",
    "        return np.log(numerator) - np.log(denominator) + 1\n",
    "    \n",
    "    return np.mean([word_class_score(w) for w in np.split(\" \")])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def map_np_char_name(nps: list[str], face_predictions: list[str], class_np_counter: Counter, np_class_counter: Counter, decided_class_np:Counter):\n",
    "    # create a dataframe to save the score of each noun phrase with the class predicted\n",
    "    np_scores = pd.DataFrame(data=[], index=nps, columns=[face_predictions])\n",
    "    for np in nps:\n",
    "        for face_pred in face_predictions:\n",
    "            np_scores.at[np, face_pred] = np_class_score(np, face_pred, class_np_counter, np_class_counter, decided_class_np)\n",
    "\n",
    "    mapping = {}\n",
    "    while not np_scores.empty:\n",
    "            \n",
    "        best_res, best_index, best_face_pred = -10, None, None\n",
    "        for face_pred in face_predictions:\n",
    "            index_value = np_scores[[face_pred]].idmax().iloc[0]\n",
    "            if np_scores[index_value, face_pred] > best_res:\n",
    "                best_index = index_value\n",
    "                best_face_pred = face_pred\n",
    "        # map the best_face_pred to the best_index\n",
    "        mapping[best_face_pred] = best_index\n",
    "        # remove the index and the face from np_scores\n",
    "        np_scores.drop(columns=best_face_pred, inplace=True)\n",
    "        np_scores.drop(index=best_index, inplace=True)\n",
    "\n",
    "    return mapping    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_with_char_names(captions: list[str], face_predictions: list[list[list[str, np.array]]]):\n",
    "    # first off extract plain names \n",
    "    face_predictions = char_names_predictions(face_predictions)\n",
    "    # extract the nps from the captions\n",
    "    nps = extract_NP_text(\". \".join(captions))\n",
    "    # first extract the captions as plain text\n",
    "    plain_text_nps = [convert_to_text(t, filter=False) for t in nps]\n",
    "    # extract the filtered version of each caption\n",
    "    filtered_text_nps = [convert_to_text(t, filter=True) for t in nps]\n",
    "    # now we have the captions and the predictions ready \n",
    "    # time to build the matrix\n",
    "    class_np_counter, np_class_counter, _ = build_captions_class_matrix(filtered_text_nps, face_predictions)\n",
    "    # find the decided captions\n",
    "    decisive_class_np_counter = find_decided_captions(filtered_text_nps, face_predictions)\n",
    "    # iterate through each of the predictions and captions\n",
    "    final_captions = []\n",
    "    for np_list_index, (np_list, pred_list) in enumerate(zip(filtered_text_nps, face_predictions)):\n",
    "        if len(np_list) != len(pred_list):\n",
    "            final_captions.append(captions[np_list_index])\n",
    "        else:\n",
    "            original_plain_text = plain_text_nps[np_list_index].copy()\n",
    "            mapping = map_np_char_name(np_list, pred_list, class_np_counter, np_class_counter, decisive_class_np_counter)\n",
    "            for np, face_pred in zip(mapping):\n",
    "                np_index = np_list.index(np)\n",
    "                # extract the original sentence to which the filtered belongs\n",
    "                original_plain_text[np_index] = face_pred\n",
    "            final_captions.append(original_plain_text)\n",
    "\n",
    "    return final_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2 = os.path.join(HOME, 'frames')\n",
    "frames2 = os.listdir(f2)\n",
    "frames2 = sorted([f for f in frames2 if not os.path.basename(f).startswith('0')], key=lambda x: int(x[:-4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "results = [get_result_image(os.path.join(f2, frames2[i])) for i in range(1, 25)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
