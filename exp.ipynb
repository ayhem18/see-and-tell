{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bouab\\DEV\\see-and-tell\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-30 14:52:47 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| constituency | wsj       |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2023-06-30 14:52:48 INFO: Using device: cuda\n",
      "2023-06-30 14:52:48 INFO: Loading: tokenize\n",
      "2023-06-30 14:52:59 INFO: Loading: pos\n",
      "2023-06-30 14:53:00 INFO: Loading: lemma\n",
      "2023-06-30 14:53:00 INFO: Loading: constituency\n",
      "2023-06-30 14:53:01 INFO: Loading: depparse\n",
      "2023-06-30 14:53:02 INFO: Loading: sentiment\n",
      "2023-06-30 14:53:02 INFO: Loading: ner\n",
      "2023-06-30 14:53:03 INFO: Done loading processors!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import stanza\n",
    "import os\n",
    "HOME = os.getcwd()\n",
    "print(HOME)\n",
    "\n",
    "from stanza.pipeline.core import DownloadMethod\n",
    "NLP = stanza.Pipeline('en', download_method=DownloadMethod.REUSE_RESOURCES, use_gpu=True) # to avoid downloading the models every time\n",
    "import torch\n",
    "\n",
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "with open('src/TBBT_embeddings_16_160_final.json', 'rb') as f:\n",
    "    bbt_embeddings = json.load(f)\n",
    "\n",
    "path_frames = os.path.join('src', 'frames')\n",
    "# delete images that start with 0\n",
    "fs = os.listdir(path_frames)\n",
    "for f in fs:\n",
    "    if not f.startswith('0'):\n",
    "        os.remove(os.path.join(path_frames, f))\n",
    "\n",
    "frames = [os.path.join(path_frames, f) for f in os.listdir(path_frames)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['penny']\n",
      "'NoneType' object is not iterable\n",
      "[]\n",
      "[]\n",
      "['sheldon']\n",
      "['sheldon']\n",
      "['penny', 'howard']\n",
      "['leonard']\n",
      "['amy']\n",
      "['leonard']\n",
      "['amy']\n"
     ]
    }
   ],
   "source": [
    "from src.face.face_recognition import recognize_faces\n",
    "for f in frames[10:20]:\n",
    "    preds = recognize_faces(f, bbt_embeddings, display=False)\n",
    "    print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.experimental.exp import get_caption\n",
    "from src.face.face_recognition import recognize_faces\n",
    "def get_result_image(image_path):\n",
    "    # extract the caption\n",
    "    caption = get_caption(image_path, use_gpu=True)\n",
    "    # get the faces present in the image \n",
    "    o1 = recognize_faces(image_path, embeddings=bbt_embeddings, display=False)    \n",
    "    print(caption)\n",
    "    print(o1)\n",
    "    return caption, o1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "HOME = os.getcwd()\n",
    "import json\n",
    "with open(os.path.join(HOME,'src','TBBT_embeddings_16_160.json')) as f:\n",
    "  bbt_embeddings = json.load(f)\n",
    "\n",
    "import os\n",
    "f = os.path.join(HOME, 'src', 'frames')\n",
    "frames = os.listdir(f)\n",
    "frames = sorted([os.path.join(f, x) for x in frames], key=lambda x: int(os.path.basename(x)[:-4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('all_captions_1.pickle', 'rb') as handle:\n",
    "    captions = pickle.load(handle)\n",
    "\n",
    "\n",
    "with open('all_predictions_1.pickle', 'rb') as handle:\n",
    "    face_preds = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "# with open('all_predictions_1.pickle', 'wb') as handle:\n",
    "#     pickle.dump(face_pred, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('all_captions_1.pickle', 'rb') as handle:\n",
    "#     captions = pickle.load(handle)\n",
    "\n",
    "\n",
    "# with open('all_predictions_1.pickle', 'rb') as handle:\n",
    "#     face_preds = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.face.face_recognition import recognize_faces\n",
    "# face_pred = [recognize_faces(os.path.join(f), embeddings=bbt_embeddings) for f in frames[1:]]\n",
    "assert len(face_preds) == len(captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_preds = [[p[0] for p in ps] for ps in face_preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.captions.captions_improved as ci\n",
    "gcs = ci.generate_captions(captions, face_preds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test refactor Face Reco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path_frames = os.path.join('src', 'frames copy')\n",
    "frames = [os.path.join(path_frames, f) for f in os.listdir(path_frames)]\n",
    "\n",
    "dir_embs = os.path.join('src', 'emd')\n",
    "\n",
    "os.makedirs(dir_embs, exist_ok=True)\n",
    "\n",
    "save_emd_file = os.path.join(dir_embs, f'{len(os.listdir(dir_embs))}.json')\n",
    "\n",
    "BBT_DIR = os.path.join('src', 'dataset', 'BigBangTheory')\n",
    "\n",
    "\n",
    "import src.face_2.embeddings as e\n",
    "import importlib\n",
    "importlib.reload(e)\n",
    "from src.face_2.embeddings import build_classes_embeddings\n",
    "\n",
    "bbt = build_classes_embeddings(directory=BBT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
