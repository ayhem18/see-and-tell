{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install nltk\n",
    "# ! pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bouab\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\cntell-xeiog5pY-py3.10\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bouab\\DEV\\see-and-tell\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-06 19:03:30 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| constituency | wsj       |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2023-05-06 19:03:31 INFO: Using device: cuda\n",
      "2023-05-06 19:03:31 INFO: Loading: tokenize\n",
      "2023-05-06 19:03:35 INFO: Loading: pos\n",
      "2023-05-06 19:03:35 INFO: Loading: lemma\n",
      "2023-05-06 19:03:35 INFO: Loading: constituency\n",
      "2023-05-06 19:03:36 INFO: Loading: depparse\n",
      "2023-05-06 19:03:36 INFO: Loading: sentiment\n",
      "2023-05-06 19:03:37 INFO: Loading: ner\n",
      "2023-05-06 19:03:38 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "import os\n",
    "HOME = os.getcwd()\n",
    "print(HOME)\n",
    "\n",
    "from stanza.pipeline.core import DownloadMethod\n",
    "NLP = stanza.Pipeline('en', download_method=DownloadMethod.REUSE_RESOURCES, use_gpu=True) # to avoid downloading the models every time\n",
    "# nlp is a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['The woman in the garden is reading a book next to her husband', 'A man in a blue shirt is talking to another man', \"A young girl is having breakfast with her mother\"]\n",
    "final_str = \". \".join(sentences)\n",
    "# let's see how things go now !!\n",
    "doc = NLP(final_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = doc.sentences\n",
    "tree = sentences[0].constituency\n",
    "c = tree.children[0].children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first 2 in general are a NP and VP\n",
    "# let's check the first component: NP\n",
    "np1 = c[0]\n",
    "res = stanza.models.constituency.parse_tree.Tree.get_compound_constituents([np1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Tree in module stanza.models.constituency.parse_tree:\n",
      "\n",
      "class Tree(stanza.models.common.stanza_object.StanzaObject)\n",
      " |  Tree(label=None, children=None)\n",
      " |  \n",
      " |  A data structure to represent a parse tree\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Tree\n",
      " |      stanza.models.common.stanza_object.StanzaObject\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __format__(self, spec)\n",
      " |      Turn the tree into a string representing the tree\n",
      " |      \n",
      " |      Note that this is not a recursive traversal\n",
      " |      Otherwise, a tree too deep might blow up the call stack\n",
      " |      \n",
      " |      There is a type specific format:\n",
      " |        O       -> one line PTB format, which is the default anyway\n",
      " |        L       -> open and close brackets are labeled, spaces in the tokens are replaced with _\n",
      " |        P       -> pretty print over multiple lines\n",
      " |        V       -> surround lines with <s>...</s>, don't print ROOT, and turn () into L/RBKT\n",
      " |        ?       -> spaces in the tokens are replaced with ? for any value of ? other than OLP\n",
      " |                   warning: this may be removed in the future\n",
      " |        ?{OLPV} -> specific format AND a custom space replacement\n",
      " |        Vi      -> add an ID to the <s> in the V format.  Also works with ?Vi\n",
      " |  \n",
      " |  __init__(self, label=None, children=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  all_leaves_are_preterminals(self)\n",
      " |      Returns True if all leaves are under preterminals, False otherwise\n",
      " |  \n",
      " |  count_unary_depth(self)\n",
      " |  \n",
      " |  depth(self)\n",
      " |  \n",
      " |  is_leaf(self)\n",
      " |  \n",
      " |  is_preterminal(self)\n",
      " |  \n",
      " |  leaf_labels(self)\n",
      " |      Get the labels of the leaves\n",
      " |  \n",
      " |  pretty_print(self, normalize=None)\n",
      " |      Print with newlines & indentation on each line\n",
      " |      \n",
      " |      Preterminals and nodes with all preterminal children go on their own line\n",
      " |      \n",
      " |      You can pass in your own normalize() function.  If you do,\n",
      " |      make sure the function updates the parens to be something\n",
      " |      other than () or the brackets will be broken\n",
      " |  \n",
      " |  prune_none(self)\n",
      " |      Return a copy of the tree, eliminating all nodes which are in one of two categories:\n",
      " |          they are a preterminal -NONE-, such as appears in PTB\n",
      " |            *E* shows up in a VLSP dataset\n",
      " |          they have been pruned to 0 children by the recursive call\n",
      " |  \n",
      " |  remap_constituent_labels(self, label_map)\n",
      " |      Copies the tree with some labels replaced.\n",
      " |      \n",
      " |      Labels in the map are replaced with the mapped value.\n",
      " |      Labels not in the map are unchanged.\n",
      " |  \n",
      " |  remap_words(self, word_map)\n",
      " |      Copies the tree with some labels replaced.\n",
      " |      \n",
      " |      Labels in the map are replaced with the mapped value.\n",
      " |      Labels not in the map are unchanged.\n",
      " |  \n",
      " |  replace_words(self, words)\n",
      " |      Replace all leaf words with the words in the given list (or iterable)\n",
      " |      \n",
      " |      Returns a new tree\n",
      " |  \n",
      " |  reverse(self)\n",
      " |      Flip a tree backwards\n",
      " |      \n",
      " |      The intent is to train a parser backwards to see if the\n",
      " |      forward and backwards parsers can augment each other\n",
      " |  \n",
      " |  simplify_labels(self, pattern=re.compile('[-=#]'))\n",
      " |      Return a copy of the tree with the -=# removed\n",
      " |      \n",
      " |      Leaves the text of the leaves alone.\n",
      " |  \n",
      " |  visit_preorder(self, internal=None, preterminal=None, leaf=None)\n",
      " |      Visit the tree in a preorder order\n",
      " |      \n",
      " |      Applies the given functions to each node.\n",
      " |      internal: if not None, applies this function to each non-leaf, non-preterminal node\n",
      " |      preterminal: if not None, applies this functiion to each preterminal\n",
      " |      leaf: if not None, applies this function to each leaf\n",
      " |      \n",
      " |      The functions should *not* destructively alter the trees.\n",
      " |      There is no attempt to interpret the results of calling these functions.\n",
      " |      Rather, you can use visit_preorder to collect stats on trees, etc.\n",
      " |  \n",
      " |  yield_preterminals(self)\n",
      " |      Yield the preterminals one at a time in order\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  get_common_words(trees, num_words)\n",
      " |      Walks over all of the trees and gets the most frequently occurring words.\n",
      " |  \n",
      " |  get_compound_constituents(trees, separate_root=False)\n",
      " |  \n",
      " |  get_constituent_counts(trees)\n",
      " |      Walks over all of the trees and gets the count of the unique constituent names from the trees\n",
      " |  \n",
      " |  get_rare_words(trees, threshold=0.05)\n",
      " |      Walks over all of the trees and gets the least frequently occurring words.\n",
      " |      \n",
      " |      threshold: choose the bottom X percent\n",
      " |  \n",
      " |  get_root_labels(trees)\n",
      " |  \n",
      " |  get_unique_constituent_labels(trees)\n",
      " |      Walks over all of the trees and gets all of the unique constituent names from the trees\n",
      " |  \n",
      " |  get_unique_tags(trees)\n",
      " |      Walks over all of the trees and gets all of the unique tags from the trees\n",
      " |  \n",
      " |  get_unique_words(trees)\n",
      " |      Walks over all of the trees and gets all of the unique words from the trees\n",
      " |  \n",
      " |  write_treebank(trees, out_file, fmt='{}')\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from stanza.models.common.stanza_object.StanzaObject:\n",
      " |  \n",
      " |  add_property(name, default=None, getter=None, setter=None) from builtins.type\n",
      " |      Add a property accessible through self.{name} with underlying variable self._{name}.\n",
      " |      Optionally setup a setter as well.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from stanza.models.common.stanza_object.StanzaObject:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(stanza.models.constituency.parse_tree.Tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('VBZ', 'is'), ('VBG', 'reading'), ('DT', 'a'), ('NN', 'book'), ('RB', 'next'), ('IN', 'to'), ('PRP$', 'her'), ('NN', 'husband')]\n",
      "[('VBG', 'reading'), ('DT', 'a'), ('NN', 'book'), ('RB', 'next'), ('IN', 'to'), ('PRP$', 'her'), ('NN', 'husband')]\n"
     ]
    }
   ],
   "source": [
    "# let's try to understand how to identify expressions that could represent humans\n",
    "from nltk.corpus import wordnet as wn\n",
    "PEOPLE = wn.synset('people.n.01')\n",
    "PERSON = wn.synset('person.n.01')\n",
    "\n",
    "def words_tags(tree):\n",
    "    return [(x.label, x.children[0].label) for x in tree.yield_preterminals()]\n",
    "\n",
    "print(words_tags(c[1]))\n",
    "print(words_tags(c[1].children[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def person_word(word:str, threshold: float=0.2, synset_reference=None) -> bool:\n",
    "    # this function will return 2 boolean values:\n",
    "    # first if the word could possibly mean the word \"person\"     \n",
    "\n",
    "    # first extract the possible meanings of such word\n",
    "    if synset_reference is None:\n",
    "        synset_reference = PERSON # this way we can use PEOPLE as well with the same function\n",
    "\n",
    "    is_person = False\n",
    "    for meaning in wn.synsets(word):\n",
    "        if meaning.path_similarity(PERSON) >= threshold:\n",
    "            is_person = True\n",
    "            break\n",
    "    \n",
    "    return is_person\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a woman who has given birth to a child (also used as a term of address to your mother)\n",
      "0.14285714285714285\n",
      "a stringy slimy substance consisting of yeast cells and bacteria; forms during fermentation and is added to cider or wine to produce vinegar\n",
      "0.125\n",
      "a term of address for an elderly woman\n",
      "0.2\n",
      "a term of address for a mother superior\n",
      "0.16666666666666666\n",
      "a condition that is the inspiration for an activity or situation\n",
      "0.09090909090909091\n",
      "care for like a mother\n",
      "0.1\n",
      "make children\n",
      "0.125\n"
     ]
    }
   ],
   "source": [
    "for meaning in wn.synsets('mother'):\n",
    "    print(meaning.definition())\n",
    "    print(meaning.path_similarity(PERSON))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(word: str , tag: str):\n",
    "    try:\n",
    "        pos = tag.lower()[0]\n",
    "        lemmas = wn._morphy(word, pos)\n",
    "        return min(lemmas, key=len) if lemmas else word\n",
    "    except:\n",
    "        # assume it is a noun\n",
    "        lemmas = wn._morphy(word, 'n')\n",
    "        return min(lemmas, key=len) if lemmas else word\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_person_words(noun_phrase: list[tuple[str, str]], known_person_words: set):\n",
    "    # first convert all the words to their lemmas\n",
    "    lemmas = [(t[0], lemmatize(t[1], t[0])) for t in noun_phrase]    \n",
    "    person_lists = [t[1] for t in lemmas if t[0] == 'NN' and (t[1] in known_person_words or \n",
    "                    person_word(t[1], synset_reference=PERSON) or person_word(t[1], synset_reference=PEOPLE))]\n",
    "\n",
    "    # make sure to add the words that could possibly mean PERSON to the set\n",
    "    known_person_words.update(person_lists)\n",
    "    return person_lists, known_person_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_NP_components(root, person_np: set=None):\n",
    "    # this function is supposed to return the largest NPs including exactly one word with a close meaning to 'PERSON'  \n",
    "    if person_np is None:\n",
    "        person_np = set()\n",
    "    \n",
    "    result = []\n",
    "    # only consider Noun Phrases\n",
    "    if root.label == 'NP':\n",
    "        # if the current root reprsents a Noun Phrase, \n",
    "        noun_phrase = words_tags(root)\n",
    "        # extract the number of words that represent a PERSON\n",
    "        num_persons, person_np = num_person_words(noun_phrase, person_np)\n",
    "\n",
    "        if len(num_persons) == 1:\n",
    "            result.append(noun_phrase)\n",
    "        \n",
    "        elif len(num_persons) >= 2: \n",
    "            for child in root.children:\n",
    "                result.extend(get_NP_components(child, person_np))\n",
    "\n",
    "    else:\n",
    "        for child in root.children:\n",
    "            result.extend(get_NP_components(child, person_np))\n",
    "\n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_text(l: list[tuple[str, str]], filter):\n",
    "    if filter:\n",
    "        return \" \".join([t[1] for t in l if t[0] in ['NN', 'JJ']]).strip().lower() \n",
    "    return \" \".join([t[1] for t in l]).strip().lower()\n",
    "\n",
    "\n",
    "def extract_NP_text(text: str, nlp_object=None, plain_text:bool=True, filter:bool=True):\n",
    "    if nlp_object is None:\n",
    "        nlp_object = NLP\n",
    "    \n",
    "    doc = nlp_object(text)\n",
    "    np_components = []\n",
    "    person_words = set()\n",
    "    # iterate through sentences\n",
    "    for s in doc.sentences:\n",
    "        tree = s.constituency\n",
    "        c = tree.children[0]\n",
    "        np_components.append(get_NP_components(c, person_words))\n",
    "\n",
    "    if plain_text:\n",
    "        return [[convert_to_text(t, filter=filter) for t in component] for component in np_components]\n",
    "    \n",
    "    return np_components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The woman in the garden is reading a book next to her husband. A man in a blue shirt is talking to another man. A young girl is having breakfast with her mother. One boy and one man were hitting an old woman\n",
      "[['woman garden', 'husband'], ['man blue shirt', 'man'], ['young girl', 'mother'], ['boy', 'man', 'old woman']]\n"
     ]
    }
   ],
   "source": [
    "sentences = ['The woman in the garden is reading a book next to her husband', \n",
    "             'A man in a blue shirt is talking to another man', \n",
    "             \"A young girl is having breakfast with her mother\", \n",
    "             \"One boy and one man were hitting an old woman\"]\n",
    "final_str = \". \".join(sentences)\n",
    "print(final_str)\n",
    "results = extract_NP_text(final_str, plain_text=True, filter=True)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for res, s in zip(results, sentences):\n",
    "#     print(s)\n",
    "#     print(\"Person Noun Phrases\")\n",
    "#     for np in res:\n",
    "#         print(\" \".join([w[1] for w in np]))\n",
    "\n",
    "#     print(\"#\" * 100)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "HOME = os.getcwd()\n",
    "import json\n",
    "with open(os.path.join(HOME,'src','TBBT_embeddings_16_256.json')) as f:\n",
    "  bbt_embeddings = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.experimental.exp import get_caption\n",
    "from src.face.face_recognition import recognize_faces\n",
    "def get_result_image(image_path):\n",
    "    # extract the caption\n",
    "    caption = get_caption(image_path)\n",
    "    # get the faces present in the image \n",
    "    o1 = recognize_faces(image_path, embeddings=bbt_embeddings, possible_classes=list(bbt_embeddings.keys()), display=False)    \n",
    "    print(caption)\n",
    "    print(o1)\n",
    "    return caption, o1\n",
    "import os\n",
    "\n",
    "f = os.path.join(HOME, 'src', 'frames')\n",
    "frames = os.listdir(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = [os.path.join(f, frames[i]) for i in range()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2 = os.path.join(HOME, 'src', 'frames')\n",
    "frames2 = os.listdir(f2)\n",
    "frames2 = sorted([f for f in frames2 if not os.path.basename(f).startswith('0')], key=lambda x: int(x[:-4]))\n",
    "\n",
    "images = [os.path.join(f2, frames2[i]) for i in range(1, 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preds = get_caption(images, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# stop\n",
    "results = [get_result_image(os.path.join(f, frames[i])) for i in range(0, 40, 2)]\n",
    "# let's take 20 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions, preds = list(map(list, zip(*results)))\n",
    "# let's save the results\n",
    "# with open( \"captions.p\", \"wb\" ) as f:\n",
    "# \tpickle.dump(captions, f)\n",
    "\n",
    "# with open( \"preds.p\", \"wb\" ) as f:\n",
    "# \tpickle.dump(preds, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def char_names_predictions(face_predictions: list[list[list[str, np.array]]]):\n",
    "    # first extract only the character names as needed\n",
    "    char_names  = [[l[0] for l in p] for p in face_predictions]\n",
    "    return char_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_captions_class_matrix(captions: list[list[str]], face_predictions: list[list[str]]):\n",
    "    class_np_counter = Counter() # to save the frequency of each np with each class\n",
    "    np_components = set()\n",
    "    np_class_counter = {} # to save which classes each np was seen with\n",
    "    for cap_list, pred_list in captions, face_predictions:\n",
    "        if len(cap_list) == len(pred_list):\n",
    "            # iterate through both captions and predictions and\n",
    "            for cap, pred in zip(cap_list, pred_list):\n",
    "                # first add each part of the caption to the np_components\n",
    "                np_components.update(np_components.split(\" \"))\n",
    "                if pred not in class_np_counter:\n",
    "                    class_np_counter[pred] = Counter() # a dictionary for each of the classes        \n",
    "                # increase the frequency of each term in the caption, in the dictionary associated with the class\n",
    "                class_np_counter[pred].update(dict([(word, 1) for word in cap.split(\" \")]))\n",
    "                # add the class to the occurences of each of the np in the captions\n",
    "                for np in cap.split(\" \"):\n",
    "                    if np not in np_class_counter:\n",
    "                        np_class_counter[np] = set() \n",
    "                    np_class_counter[np].add(pred)\n",
    "            \n",
    "    return class_np_counter, np_class_counter, np_components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def find_decided_captions(captions: list[str], face_predictions: list[list[str]]):\n",
    "    # this function will just return the captions and face_predictions with length 1\n",
    "    temp_list =  [(c[0], fp[0]) for c, fp in zip(captions, face_predictions) if len(c) == len(fp) == 1]\n",
    "    # convert the list of tuples to 2 lists\n",
    "    captions, predictions = list(map(list, zip(*temp_list)))\n",
    "    # build a counter to map each class to its decided captions\n",
    "    decisive_class_np_counter = Counter()\n",
    "    for c, pred in zip(captions, predictions):\n",
    "        if pred not in decisive_class_np_counter:\n",
    "            decisive_class_np_counter[pred] = Counter()\n",
    "        decisive_class_np_counter[pred].update(dict([(w, 1) for w in c.split(\" \")]))\n",
    "    return decisive_class_np_counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def np_class_score(np: str, class_prediction: str, class_np_counter: Counter, np_class_counter: Counter, decided_class_np: Counter):\n",
    "    def word_class_score(word: str):\n",
    "        # first let's build the numerator:1 + the number of occurences of the word with the class pred + the number of times it was seen in a decisive prediction\n",
    "        numerator = 1 + (class_np_counter[class_prediction][word] if word in class_np_counter[class_prediction] else 0) + \\\n",
    "                        (decided_class_np[class_prediction][word] if word in decided_class_np else 0)\n",
    "        # the denominator: the number of unique classes the word was associated with + 1 \n",
    "        denominator = 1 + len(np_class_counter[word]) \n",
    "\n",
    "        return np.log(numerator) - np.log(denominator) + 1\n",
    "    \n",
    "    return np.mean([word_class_score(w) for w in np.split(\" \")])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def map_np_char_name(nps: list[str], face_predictions: list[str], class_np_counter: Counter, np_class_counter: Counter, decided_class_np:Counter):\n",
    "    # create a dataframe to save the score of each noun phrase with the class predicted\n",
    "    np_scores = pd.DataFrame(data=[], index=nps, columns=[face_predictions])\n",
    "    for np in nps:\n",
    "        for face_pred in face_predictions:\n",
    "            np_scores.at[np, face_pred] = np_class_score(np, face_pred, class_np_counter, np_class_counter, decided_class_np)\n",
    "\n",
    "    mapping = {}\n",
    "    while not np_scores.empty:\n",
    "            \n",
    "        best_res, best_index, best_face_pred = -10, None, None\n",
    "        for face_pred in face_predictions:\n",
    "            index_value = np_scores[[face_pred]].idmax().iloc[0]\n",
    "            if np_scores[index_value, face_pred] > best_res:\n",
    "                best_index = index_value\n",
    "                best_face_pred = face_pred\n",
    "        # map the best_face_pred to the best_index\n",
    "        mapping[best_face_pred] = best_index\n",
    "        # remove the index and the face from np_scores\n",
    "        np_scores.drop(columns=best_face_pred, inplace=True)\n",
    "        np_scores.drop(index=best_index, inplace=True)\n",
    "\n",
    "    return mapping    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_with_char_names(captions: list[str], face_predictions: list[list[list[str, np.array]]]):\n",
    "    # first off extract plain names \n",
    "    face_predictions = char_names_predictions(face_predictions)\n",
    "    # extract the nps from the captions\n",
    "    nps = extract_NP_text(\". \".join(captions))\n",
    "    # first extract the captions as plain text\n",
    "    plain_text_nps = [convert_to_text(t, filter=False) for t in nps]\n",
    "    # extract the filtered version of each caption\n",
    "    filtered_text_nps = [convert_to_text(t, filter=True) for t in nps]\n",
    "    # now we have the captions and the predictions ready \n",
    "    # time to build the matrix\n",
    "    class_np_counter, np_class_counter, _ = build_captions_class_matrix(filtered_text_nps, face_predictions)\n",
    "    # find the decided captions\n",
    "    decisive_class_np_counter = find_decided_captions(filtered_text_nps, face_predictions)\n",
    "    # iterate through each of the predictions and captions\n",
    "    final_captions = []\n",
    "    for np_list_index, (np_list, pred_list) in enumerate(zip(filtered_text_nps, face_predictions)):\n",
    "        if len(np_list) != len(pred_list):\n",
    "            final_captions.append(captions[np_list_index])\n",
    "        else:\n",
    "            original_plain_text = plain_text_nps[np_list_index].copy()\n",
    "            mapping = map_np_char_name(np_list, pred_list, class_np_counter, np_class_counter, decisive_class_np_counter)\n",
    "            for np, face_pred in zip(mapping):\n",
    "                np_index = np_list.index(np)\n",
    "                # extract the original sentence to which the filtered belongs\n",
    "                original_plain_text[np_index] = face_pred\n",
    "            final_captions.append(original_plain_text)\n",
    "\n",
    "    return final_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2 = os.path.join(HOME, 'frames')\n",
    "frames2 = os.listdir(f2)\n",
    "frames2 = sorted([f for f in frames2 if not os.path.basename(f).startswith('0')], key=lambda x: int(x[:-4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "results = [get_result_image(os.path.join(f2, frames2[i])) for i in range(1, 25)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
